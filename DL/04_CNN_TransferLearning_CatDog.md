# Cats vs Dogs ë¶„ë¥˜ ì‹¤ìŠµ (DNN, CNN, ì „ì´í•™ìŠµ, ì´ë¯¸ì§€ ì¦ê°•)

## ğŸ“Œ ëª©í‘œ

-   ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°ë¥¼ ì œì‘í•œë‹¤.
-   ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ê¸°ë²•ì„ ì ìš©í•´ ì„±ëŠ¥ í–¥ìƒì„ ì‹¤í—˜í•œë‹¤.
    -   DNN, CNN, ì´ë¯¸ì§€ ì¦ê°•, ì „ì´í•™ìŠµ(VGG16), ë¯¸ì„¸ì¡°ì •(Fine-tuning)
    -   ê³¼ì í•© ë°©ì§€ ê¸°ë²•: Dropout, BatchNormalization, GlobalAveragePooling

---

## ğŸ“ 1. Colab í™˜ê²½ ì¤€ë¹„ ë° ë°ì´í„° ë¡œë”©

### ğŸ“‚ Google Drive ì—°ë™

```python
from google.colab import drive
drive.mount('/content/drive')
%cd /content/drive/MyDrive/Colab Notebooks/deeplearning_2025

```

### ğŸ“¥ ë°ì´í„° ë¡œë”©

-   ì´ì „ì— `.npz` í˜•íƒœë¡œ ì €ì¥í•œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.

```python
import numpy as np

data = np.load("./data/np_cat_vs_dog.npz")
X_train, X_test = data["X_train"], data["X_test"]
y_train, y_test = data["y_train"], data["y_test"]

```

---

## ğŸ§± 2. DNN ëª¨ë¸ ì„¤ê³„

### âœ… êµ¬ì„±

-   ì…ë ¥ì¸µ: `Flatten`ì„ í†µí•´ 1Dë¡œ ë³€í™˜
-   ì€ë‹‰ì¸µ: Dense(256) â†’ Dense(128) â†’ Dense(64)
-   ì¶œë ¥ì¸µ: `sigmoid` (ì´ì§„ ë¶„ë¥˜)

### ğŸ›  ì½”ë“œ

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense, Flatten
from tensorflow.keras.callbacks import EarlyStopping

model1 = Sequential([
    InputLayer(input_shape=(224, 224, 3)),
    Flatten(),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

es = EarlyStopping(monitor='val_accuracy', patience=5)
hist1 = model1.fit(X_train, y_train, epochs=10, validation_split=0.3, callbacks=[es])

```

---

## ğŸ“ˆ 3. ì„±ëŠ¥ ì‹œê°í™”

```python
import matplotlib.pyplot as plt

plt.plot(hist1.history["accuracy"], label="Train acc")
plt.plot(hist1.history["val_accuracy"], label="Validation acc")
plt.legend(); plt.show()

```

---

## ğŸ§± 4. CNN ëª¨ë¸ ì„¤ê³„

### âœ… CNN ê¸°ë³¸ êµ¬ì¡°

-   Conv2D â†’ MaxPooling2D Ã— 3
-   Flatten â†’ Dense Ã— 3 â†’ Output

```python
from tensorflow.keras.layers import Conv2D, MaxPooling2D

model2 = Sequential([
    InputLayer(input_shape=(224, 224, 3)),
    Conv2D(32, (3, 3), activation="relu", padding="same"),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation="relu", padding="same"),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation="relu", padding="same"),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(256, activation="relu"),
    Dense(128, activation="relu"),
    Dense(64, activation="relu"),
    Dense(1, activation="sigmoid")
])

model2.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
hist2 = model2.fit(X_train, y_train, epochs=2, validation_split=0.3, callbacks=[es])

```

---

## ğŸ–¼ï¸ 5. ì´ë¯¸ì§€ ì¦ê°•(Augmentation)

### âœ… ëª©ì 

-   ë°ì´í„° ìˆ˜ê°€ ë¶€ì¡±í•  ë•Œ ë‹¤ì–‘í•œ ë³€í˜•ì„ í†µí•´ ë°ì´í„° ìˆ˜ë¥¼ ëŠ˜ë¦°ë‹¤
-   íšŒì „, ì´ë™, ê¸°ìš¸ê¸°, í™•ëŒ€, ë°˜ì „ ë“±ì„ ì ìš©í•œë‹¤

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode="nearest"
)

test_gen = ImageDataGenerator(rescale=1./255)

train_generator = train_gen.flow_from_directory(
    "./data/cats_and_dogs_filtered/train", target_size=(224, 224),
    batch_size=20, class_mode="binary"
)

test_generator = test_gen.flow_from_directory(
    "./data/cats_and_dogs_filtered/validation", target_size=(224, 224),
    batch_size=20, class_mode="binary"
)

```

---

## ğŸ§± 6. ì¦ê°• ë°ì´í„° ê¸°ë°˜ CNN í•™ìŠµ

```python
model3 = Sequential([
    InputLayer(input_shape=(224, 224, 3)),
    Conv2D(32, (3,3), activation="relu", padding="same"),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation="relu", padding="same"),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation="relu", padding="same"),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(256, activation="relu"),
    Dense(128, activation="relu"),
    Dense(64, activation="relu"),
    Dense(1, activation="sigmoid")
])

model3.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
hist3 = model3.fit(train_generator, validation_data=test_generator, epochs=2, callbacks=[es])

```

---

## ğŸ” 7. ì „ì´í•™ìŠµ (Transfer Learning) - VGG16

### âœ… íŠ¹ì§• ì¶”ì¶œ ë°©ì‹ (ë™ê²°)

```python
from keras.applications import VGG16

conv_base = VGG16(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
conv_base.trainable = False  # ì „ì²˜ë¦¬ê¸° ê°€ì¤‘ì¹˜ ë™ê²°

model4 = Sequential([
    conv_base,
    Flatten(),
    Dense(256, activation="relu"),
    Dense(128, activation="relu"),
    Dense(64, activation="relu"),
    Dense(1, activation="sigmoid")
])

model4.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
hist4 = model4.fit(X_train, y_train, epochs=2, validation_split=0.3)

```

---

## ğŸ”§ 8. ë¯¸ì„¸ì¡°ì • (Fine-Tuning)

### âœ… block5_conv3 ì´í›„ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •

```python
conv_base.trainable = True
for layer in conv_base.layers:
    if layer.name == "block5_conv3":
        layer.trainable = True
    else:
        layer.trainable = False

model4.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
hist4 = model4.fit(X_train, y_train, epochs=2, validation_split=0.3)

```

---

## ğŸ§ª 9. ê³¼ì í•© ë°©ì§€ ëª¨ë¸ ì„¤ê³„

### âœ… BatchNormalization + LeakyReLU + GlobalAveragePooling

```python
from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, GlobalAveragePooling2D

model5 = Sequential([
    InputLayer(input_shape=(224, 224, 3)),
    Conv2D(32, (3,3), padding="same"),
    BatchNormalization(),
    Activation(LeakyReLU()),
    MaxPooling2D((2,2)),

    Conv2D(64, (3,3), padding="same"),
    BatchNormalization(),
    Activation(LeakyReLU()),
    MaxPooling2D((2,2)),

    Conv2D(64, (3,3), padding="same"),
    BatchNormalization(),
    Activation(LeakyReLU()),
    GlobalAveragePooling2D(),

    Dense(256, activation="relu"),
    Dense(128, activation="relu"),
    Dense(64, activation="relu"),
    Dense(1, activation="sigmoid")
])

model5.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
hist5 = model5.fit(X_train, y_train, epochs=10, validation_split=0.3, callbacks=[es])

```

---

## ğŸ“Š 10. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”

```python
plt.plot(hist5.history["accuracy"], label="Train acc")
plt.plot(hist5.history["val_accuracy"], label="Validation acc")
plt.legend(); plt.show()

```
