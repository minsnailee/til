# 📘 ML

---

## ✅ 머신러닝 종류

### 1. 지도학습 (Supervised Learning)

- *정답(Label)**이 있는 데이터를 바탕으로 학습
- 목표: 입력 → 출력(Label)을 예측하는 모델 생성
- 주요 유형:
    - **분류(Classification)**: 범주형 결과 예측
        - 예: 붓꽃 품종 분류, 암 양성/음성 판별
        - 이진 분류 / 다중 분류
    - **회귀(Regression)**: 연속적인 수치 예측
        - 예: 연간 소득 예측, 주택 가격 예측
- 🔍 구분 기준
    - **분류** → 범주형 데이터
    - **회귀** → 수치형 데이터

### 2. 비지도학습 (Unsupervised Learning)

- 정답(Label)이 없이 **패턴이나 구조를 찾는 학습**
- 주요 유형:
    - **클러스터링(Clustering)**: 비슷한 데이터끼리 그룹화
        - 예: 고객 세분화, 동물 이미지 자동 분류
    - **차원 축소(Dimensionality Reduction)**: 데이터 시각화나 압축
- 예시:
    - 얼굴 인식
    - 소비자 행동 분석

### 3. 강화학습 (Reinforcement Learning)

- 명확한 정답은 없고, **보상(Reward)** 을 통해 학습
- 에이전트가 환경 속에서 **최대한의 보상**을 얻는 방향으로 스스로 학습
- 주로 **게임, 로봇 제어, 자동 운전** 등에 사용



## ✅ 머신러닝 기본 과정

1. 문제 정의 (Problem Identification)
    - 비즈니스 목적 명확히 설정
    - 지도/비지도/강화학습 판단
    - 분류/회귀 판단
2. 데이터 수집 (Data Collection)
    - CSV, JSON, DB, 웹 크롤링, IoT 센서, 설문 등 다양한 소스
3. 데이터 전처리 (Data Preprocessing)
    - 결측치/이상치 처리
    - **Feature Engineering (특성 공학)**
        - Scaling (정규화, 표준화)
        - Encoding (범주형 → 숫자형)
        - Binning (숫자형 → 범주형)
        - Transform (새로운 피처 생성)
4. 탐색적 데이터 분석 (EDA)
    - 통계 분석, 변수 간 상관관계
    - 시각화 (pandas, matplotlib, seaborn 등)
5. 특성 선택 (Feature Selection)
    - 중요한 피처만 선별해 사용
6. 모델 선택 & 하이퍼파라미터 튜닝
    - 모델 예시: KNN, SVM, 선형 회귀, 결정트리, 랜덤 포레스트, CNN, RNN 등
    - **하이퍼파라미터(Hyperparameter)**
        
        → 학습 전에 사람이 지정하는 값 (예: KNN의 k값, SVM의 C값 등)
        
        → 모델 성능에 큰 영향
        
7. 학습 (Training)
    
    ```python
    model.fit(X_train, y_train)  # 학습
    model.predict(X_test)        # 예측
    ```
    
    - train/test 데이터는 일반적으로 7:3 비율로 분할
8. 평가 (Evaluation)
    - 정확도, 정밀도, 재현율, F1-score 등 사용

---

## 📌 과대적합이란?

- 모델이 **학습 데이터에 너무 잘 맞아버리는** 상황
- 학습 데이터에서는 성능이 아주 좋지만, 테스트 데이터나 실전 데이터에서는 **예측 성능이 급격히 떨어짐**
- **기억은 잘 하지만, 일반화는 못하는 학생**이라고 생각하면 쉬워

---

## 📚 구체적인 예시

### 예시: 시험 공부

- 네가 수학 시험을 준비한다고 해 보자.
- 전년도 기출문제 100문제를 달달 외웠어.
- 시험에서 **완전히 똑같은 문제가 나오면 전부 맞춤**.
- 하지만 실제 시험에서는 **문제가 조금만 바뀌면 틀림**.

👉 기출문제를 **암기만** 하고, **개념이나 원리를 이해하지 못한 상태** = **과대적합된 상태**

---

### 예시: 머신러닝 모델

- 예를 들어, 몸무게와 키로 비만을 예측하는 모델이 있다고 해.
- 학습 데이터에서는:
    - "키 170cm, 몸무게 80kg이면 반드시 비만"이라고 학습함
- 그런데 테스트 데이터에서:
    - "키 170cm, 몸무게 78kg" → 정상
    - "키 170cm, 몸무게 82kg" → 비만
    - 이럴 땐 **경계가 너무 딱딱하게 학습돼서** 예측을 잘 못하게 돼

👉 학습 데이터에선 정확도가 99%인데, 실제 데이터에선 60%도 못 나옴

---

## 🧨 과대적합이 생기는 이유

- 모델이 너무 복잡함 (예: 깊은 트리, 복잡한 신경망 등)
- 학습 데이터가 적거나 편향되어 있음
- 학습을 너무 오래 시킴 (에폭 수 과다)
- 불필요한 피처가 많아서 잡음까지 학습함

---

## 🛠️ 과대적합 방지 방법

1. **단순한 모델 사용**
2. **데이터 양 늘리기**
3. **교차 검증 (Cross Validation)**
4. **조기 종료 (Early Stopping)**
5. **정규화 (Regularization)**
6. **Dropout (신경망에서 사용)**
7. **Feature Selection** → 중요한 피처만 사용

---
## 📌 과소적합(Underfitting)이란?

- 모델이 너무 단순하거나 학습이 부족해서 **패턴을 제대로 학습하지 못한 상태**
- 학습 데이터와 테스트 데이터 모두에서 성능이 낮음
- 쉽게 말해:
    
    → **아무것도 제대로 배우지 못한 상태**
    

---

## 📚 구체적인 예시

### 예시: 시험 공부

- 수학 시험을 앞두고 공부를 거의 안 했어.
- 공식도 모르고, 예제도 안 풀어봄.
- 쉬운 문제도 틀리고, 어려운 건 당연히 못 풂.

👉 공부 자체가 부족해서 점수가 낮음 = 과소적합

---

### 예시: 머신러닝 모델

- 키와 몸무게로 비만을 예측하는 문제
- 근데 모델로 단순히 "몸무게가 70 이상이면 무조건 비만"이라는 식의 **선형 모델**을 씀
- 그런데 현실은 그렇지 않음:
    - 키가 크면 70kg도 정상일 수 있고
    - 키가 작으면 65kg도 비만일 수 있음

👉 데이터의 복잡한 패턴을 **단순한 규칙으로 표현**해서 예측이 잘 안 됨

---

## 🧨 과소적합이 생기는 이유

- 모델이 너무 단순함 (예: 선형 회귀로 복잡한 데이터 예측 시도)
- 학습 횟수 부족 (에폭 수 너무 작음)
- 너무 많은 정규화 사용
- 중요한 피처가 누락되었거나, 전처리 부족

---

## 🛠️ 과소적합 해결 방법

1. **더 복잡한 모델 사용**
    - 예: 선형 회귀 → 다항 회귀 / 결정 트리
2. **학습 더 오래 시키기**
3. **더 많은 피처 사용**
    - 의미 있는 변수를 추가
4. **정규화 약하게 조정**
5. **모델 구조 튜닝 (딥러닝이라면 층/노드 수 증가)**

---

## 📊 과대적합 vs 과소적합 vs 일반화

| 구분 | 설명 | 훈련 세트 성능 | 테스트 세트 성능 | 원인 예시 |
| --- | --- | --- | --- | --- |
| **과소적합** | 모델이 너무 단순해서 학습 데이터조차 잘 예측하지 못함 | ❌ 낮음 | ❌ 낮음 | 너무 단순한 모델, 학습 부족, 피처 부족 등 |
| **과대적합** | 학습 데이터를 너무 과하게 외워서 새로운 데이터(테스트)에 적용 못함 | ✅ 매우 높음 | ❌ 낮음 | 모델 복잡도 과다, 에폭 과다, 잡음까지 학습 |
| **일반화** | 모델이 학습한 내용이 새로운 데이터(테스트)에도 잘 작동함 | ✅ 높음 | ✅ 높음 | 적절한 복잡도, 적절한 학습, 좋은 데이터 구성 |

---

## 🔍 한눈에 비교

- **과소적합**: "공부를 너무 안 한 상태"
- **과대적합**: "기출문제만 외운 상태"
- **일반화**: "개념도 이해하고, 문제 해결 능력도 갖춘 상태"

---

## 📌 정리

- 좋은 모델은 훈련 세트뿐 아니라 **보지 못한 테스트 세트에도 잘 작동해야 함**
- 이걸 **"일반화 능력(Generalization)"이 높다**고 말함
- 과소적합은 **단순함**, 과대적합은 **과한 복잡함**이 문제

---

## 🎯 모델의 복잡도와 일반화 문제 해결 방법

모델이 너무 복잡하면 **과대적합**, 너무 단순하면 **과소적합**이 발생할 수 있음.

이 균형을 잘 맞춰야 **일반화(Generalization)**가 잘 됨.

---

### ✅ 모델 복잡도 해결 전략

1. **충분하고 다양한 데이터 확보**
    - **데이터양이 많을수록 일반화에 도움**
    - 단순히 양만 많은 것이 아닌, **다양한 상황을 반영한 데이터**가 필요
        - 예: 나이, 성별, 환경 등 여러 조건이 골고루 포함된 데이터
    - **편중된 데이터**(예: 특정 그룹만 포함된 데이터)는 오히려 일반화 성능을 떨어뜨림
2. **규제(Regularization) 사용**
    - 모델의 **복잡도를 인위적으로 줄이는 기법**
    - 학습 과정에서 **가중치 크기를 제한**해 과대적합을 방지
    - 대표적인 정규화 기법:
        - **L1 규제 (Lasso)**: 불필요한 피처는 아예 제거함
        - **L2 규제 (Ridge)**: 가중치를 작게 조정
    - 사용 예 (scikit-learn):
        
        ```python
        from sklearn.linear_model import Ridge
        model = Ridge(alpha=1.0)  # alpha가 클수록 규제가 강해짐
        
        ```
        
3. **모델 단순화 또는 구조 조정**
    - 너무 복잡한 모델은 일반화에 실패할 수 있음
    - 예: 신경망 층 수 줄이기, 트리 깊이 제한하기 등

---

### 📌 정리

- **좋은 일반화**를 위해선:
    - **데이터의 양과 다양성** 확보
    - **모델 복잡도 적절히 조절**
    - **정규화 기법 적용**이 중요