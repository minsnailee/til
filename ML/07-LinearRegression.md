ì•„ë˜ëŠ” ë³´ìŠ¤í„´ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ ì‹¤ìŠµì„ ì½”ë“œì™€ ì„¤ëª… ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. ì „ë°˜ì ì¸ íë¦„ì€ ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ í•™ìŠµ â†’ í‰ê°€ â†’ ì„±ëŠ¥ ê°œì„  â†’ ê·œì œ ì ìš© ìˆœì„œë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.

---

## ğŸ“Œ 1. ë¬¸ì œ ì •ì˜

ë³´ìŠ¤í„´ ì£¼íƒ ê°€ê²© ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ **íšŒê·€ ëª¨ë¸(Linear Regression, SGDRegressor)** ì„ í†µí•´ ì£¼íƒ ê°€ê²©(MEDV)ì„ ì˜ˆì¸¡í•œë‹¤.

---

## ğŸ“Œ 2. ë°ì´í„° ìˆ˜ì§‘ ë° ì¤€ë¹„

### (1) ë°ì´í„° ë¡œë”©

```python

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ë¡œì»¬ íŒŒì¼ ì‚¬ìš©
boston_data = pd.read_csv('./data/boston_housing.csv')

# ì…ë ¥(X), ì¶œë ¥(y) ì •ì˜
X = boston_data.loc[:, 'CRIM':'LSTAT']
y = boston_data['MEDV']

```

### (2) ì»¬ëŸ¼ ì„¤ëª… ìš”ì•½

| ì»¬ëŸ¼ | ì˜ë¯¸ |
| --- | --- |
| CRIM | ë²”ì£„ ë°œìƒë¥  |
| ZN | ëŒ€í˜• ì£¼ê±°ì§€ì—­ ë¹„ìœ¨ |
| INDUS | ë¹„ìƒì—…ì§€ì—­ ë¹„ìœ¨ |
| CHAS | ì°°ìŠ¤ê°• ì—¬ë¶€ |
| NOX | ì¼ì‚°í™”ì§ˆì†Œ ë†ë„ |
| RM | ì£¼íƒë‹¹ ë°© ìˆ˜ |
| AGE | ì˜¤ë˜ëœ ì£¼íƒ ë¹„ìœ¨ |
| DIS | ê³ ìš©ì„¼í„° ê±°ë¦¬ |
| RAD | ê³ ì†ë„ë¡œ ì ‘ê·¼ ìš©ì´ì„± |
| TAX | ì¬ì‚°ì„¸ìœ¨ |
| PTRATIO | êµì‚¬/í•™ìƒ ë¹„ìœ¨ |
| B | í‘ì¸ ë¹„ìœ¨ ì§€ìˆ˜ |
| LSTAT | ì €ì†Œë“ì¸µ ë¹„ìœ¨ |
| MEDV | ì£¼íƒ ê°€ê²© (ëª©í‘œê°’) |

---

## ğŸ“Œ 3. ë°ì´í„° ì „ì²˜ë¦¬

### (1) ìŠ¤ì¼€ì¼ë§(StandardScaler ì‚¬ìš©)

```python

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

```

---

## ğŸ“Œ 4. ë°ì´í„° ë¶„ë¦¬

```python

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=50
)

```

---

## ğŸ“Œ 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

### (1) ì„ í˜• íšŒê·€ (Linear Regression)

```python

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

print("R^2:", lr_model.score(X_test, y_test))  # 0.668
print("MSE:", mean_squared_error(y_test, lr_model.predict(X_test)))  # ì•½ 33.86

```

### (2) SGD íšŒê·€ (SGDRegressor)

```python

from sklearn.linear_model import SGDRegressor

sgd_model = SGDRegressor(eta0=0.01, max_iter=5000, verbose=1)
sgd_model.fit(X_train, y_train)

print("R^2:", sgd_model.score(X_test, y_test))  # 0.665
print("MSE:", mean_squared_error(y_test, sgd_model.predict(X_test)))  # ì•½ 34.17

```

---

## ğŸ“Œ 6. ì„±ëŠ¥ ê°œì„  - íŠ¹ì„± í™•ì¥ (Polynomial Features)

### (1) ì»¬ëŸ¼ ì¡°í•©í•˜ì—¬ ê³±í•˜ê¸°

```python

for i in range(X.columns.size):
    for j in range(i, X.columns.size):
        X[X.columns[i] + '*' + X.columns[j]] = X[X.columns[i]] * X[X.columns[j]]

```

- ì´ 104ê°œì˜ íŠ¹ì„±ìœ¼ë¡œ í™•ì¥ë¨.

### (2) ë‹¤ì‹œ ìŠ¤ì¼€ì¼ë§

```python

X_trans = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

```

### (3) ë‹¤ì‹œ ë¶„ë¦¬ í›„ í•™ìŠµ

```python

X_train, X_test, y_train, y_test = train_test_split(
    X_trans, y, test_size=0.3, random_state=60
)

lr_model2 = LinearRegression()
lr_model2.fit(X_train, y_train)

print("R^2:", lr_model2.score(X_test, y_test))  # ì•½ 0.854

```

â†’ **íŠ¹ì„± í™•ì¥**ì„ í†µí•´ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

---

## ğŸ“Œ 7. ì •ê·œí™” (ê·œì œ ëª¨ë¸ ì ìš©)

### (1) Ridge (L2 ê·œì œ)

```python

from sklearn.linear_model import Ridge

ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)

print("Train R^2:", ridge.score(X_train, y_train))  # 0.93
print("Test R^2:", ridge.score(X_test, y_test))    # 0.854

```

- ëª¨ë“  íŠ¹ì„±ì„ ì¼ì • ë¹„ìœ¨ë¡œ ì¤„ì—¬ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë° íš¨ê³¼ì ì´ë‹¤.

---

### (2) Lasso (L1 ê·œì œ)

```python

from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.001)
lasso.fit(X_train, y_train)

print("Train R^2:", lasso.score(X_train, y_train))  # 0.921
print("Test R^2:", lasso.score(X_test, y_test))    # 0.836
print("ì‚¬ìš©ëœ íŠ¹ì„± ìˆ˜:", np.sum(lasso.coef_ != 0))  # 104

```

- ì¼ë¶€ íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ **íŠ¹ì„± ì„ íƒ ê¸°ëŠ¥**ë„ í•¨ê»˜ ìˆ˜í–‰í•œë‹¤.

---

## âœ… ìµœì¢… ìš”ì•½

| ëª¨ë¸ | íŠ¹ì„± ìˆ˜ | RÂ²(í…ŒìŠ¤íŠ¸) | íŠ¹ì§• |
| --- | --- | --- | --- |
| Linear Regression | 13 | 0.668 | ê¸°ë³¸ ëª¨ë¸ |
| SGDRegressor | 13 | 0.665 | ê²½ì‚¬í•˜ê°•ë²• |
| Linear + í™•ì¥ | 104 | **0.854** | íŠ¹ì„± ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ |
| Ridge | 104 | 0.854 | ê³¼ì í•© ë°©ì§€ì— íš¨ê³¼ì  |
| Lasso | 104 | 0.836 | íŠ¹ì„± ì„ íƒ ê°€ëŠ¥ |