- ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ í•œêµ­ì–´ í˜ì˜¤í‘œí˜„ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤ìŠµ
- ë¹ˆë„ë¶„ì„, GBM ê³„ì—´ ëª¨ë¸

---

## âœ… 1. ë°ì´í„° ë¡œë“œ ë° í™•ì¸

```python
import pandas as pd

train = pd.read_csv('./data/unsmile_train_v1.0.tsv', delimiter="\t")
valid = pd.read_csv('./data/unsmile_valid_v1.0.tsv', delimiter="\t")

train.info()
valid.info()

```

ğŸ“Œ *í›ˆë ¨ ë°ì´í„°ëŠ” 15,005ê°œ, ê²€ì¦ ë°ì´í„°ëŠ” 3,737ê°œì´ë©° ê²°ì¸¡ì¹˜ëŠ” ì—†ìŒ*

---

## âœ… 2. íŠ¹ì • í˜ì˜¤í‘œí˜„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„

```python
from collections import Counter

text_train = train[train["ì§€ì—­"]==1]["ë¬¸ì¥"]
tokens = [t for doc in text_train for t in doc.split(" ")]
counter = Counter(tokens)

print(counter.most_common(20))

```

ğŸ“Œ *â€˜ì§€ì—­â€™ ê´€ë ¨ í˜ì˜¤ ë¬¸ì¥ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ë“¤ì„ í™•ì¸*

---

## âœ… 3. ì •ì œ: ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±° ë° ë‹¨ì–´ í•„í„°ë§

```python
import re

p = re.compile("[^ê°€-í£\s]")

def clean_texts(texts):
    cleaned = []
    for doc in texts:
        temp = []
        for token in doc.split(" "):
            if len(token) < 2: continue
            if p.search(token): continue
            temp.append(token)
        cleaned.append(" ".join(temp))
    return cleaned

train_clean = clean_texts(train["ë¬¸ì¥"])
valid_clean = clean_texts(valid["ë¬¸ì¥"])

```

ğŸ“Œ *í•œ ê¸€ì ë˜ëŠ” íŠ¹ìˆ˜ë¬¸ì, ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ì–´ ì œê±°*

---

## âœ… 4. ë„ì–´ì“°ê¸° êµì • (Kiwi)

```python
!pip install -q kiwipiepy konlpy
from kiwipiepy import Kiwi

kiwi = Kiwi()

train_clean2 = list(kiwi.space(train_clean))
valid_clean2 = list(kiwi.space(valid_clean))

```

ğŸ“Œ *ì˜¬ë°”ë¥¸ ë„ì–´ì“°ê¸° êµì •*

---

## âœ… 5. ë¶ˆìš©ì–´ ì²˜ë¦¬ ë° í˜•íƒœì†Œ ë¶„ì„

```python
from kiwipiepy.utils import Stopwords
from konlpy.tag import Okt

stopwords = Stopwords()
stopwords.add(("ìˆ", "VA"))  # ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì¶”ê°€
okt = Okt()

def clean_and_tokenize(text):
    tokens = kiwi.tokenize(text, stopwords=stopwords)
    clean_text = " ".join([token.form for token in tokens])
    morphs = okt.morphs(clean_text)
    return " ".join(morphs)

train_clean3 = [clean_and_tokenize(doc) for doc in train_clean2]
valid_clean3 = [clean_and_tokenize(doc) for doc in valid_clean2]

```

ğŸ“Œ *ë¶ˆìš©ì–´ ì œê±° í›„ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ í† í°í™”*

---

## âœ… 6. ë²¡í„°í™” (BoW, TF-IDF)

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

cv = CountVectorizer(ngram_range=(1,2), max_df=0.7, min_df=10)
tv = TfidfVectorizer(ngram_range=(1,2), max_df=0.7, min_df=10)

X_train_cv = cv.fit_transform(train_clean3)
X_valid_cv = cv.transform(valid_clean3)

X_train_tv = tv.fit_transform(train_clean3)
X_valid_tv = tv.transform(valid_clean3)

```

ğŸ“Œ *ë¬¸ì¥ì„ ìˆ˜ì¹˜í™”. unigram, bigram í¬í•¨ / ë„ˆë¬´ ìì£¼/ë“œë¬¼ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸*

---

## âœ… 7. ë¼ë²¨ ë°ì´í„° ìƒì„±

```python
train["í˜ì˜¤í‘œí˜„"] = train.loc[:, "ì—¬ì„±/ê°€ì¡±":"ê¸°íƒ€ í˜ì˜¤"].sum(axis=1)
valid["í˜ì˜¤í‘œí˜„"] = valid.loc[:, "ì—¬ì„±/ê°€ì¡±":"ê¸°íƒ€ í˜ì˜¤"].sum(axis=1)

def label(row):
    if row["í˜ì˜¤í‘œí˜„"] > 0:
        return 0  # í˜ì˜¤
    elif row["ì•…í”Œ/ìš•ì„¤"] > 0:
        return 1  # ì•…í”Œ
    else:
        return 2  # í´ë¦°

new_train = train[["ë¬¸ì¥", "í˜ì˜¤í‘œí˜„", "ì•…í”Œ/ìš•ì„¤", "clean"]]
new_valid = valid[["ë¬¸ì¥", "í˜ì˜¤í‘œí˜„", "ì•…í”Œ/ìš•ì„¤", "clean"]]

y_train = new_train.apply(label, axis=1)
y_valid = new_valid.apply(label, axis=1)

```

ğŸ“Œ *3ê°œ ë¼ë²¨: 0=í˜ì˜¤, 1=ìš•ì„¤, 2=í´ë¦°*

---

## âœ… 8. í•™ìŠµ ë° êµì°¨ê²€ì¦

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

lr = LogisticRegression(C=10, max_iter=1000)
rf = RandomForestClassifier(random_state=42, n_estimators=100)

# Logistic Regression + BoW / TF-IDF
print(cross_val_score(lr, X_train_cv, y_train, cv=5).mean())
print(cross_val_score(lr, X_train_tv, y_train, cv=5).mean())

# Random Forest + BoW / TF-IDF
print(cross_val_score(rf, X_train_cv, y_train, cv=5).mean())
print(cross_val_score(rf, X_train_tv, y_train, cv=5).mean())

```

ğŸ“Œ *ì„±ëŠ¥ ì¼ë°˜í™”ë¥¼ ìœ„í•œ êµì°¨ê²€ì¦ ì§„í–‰*

---

## âœ… 9. GBM ê³„ì—´ ëª¨ë¸ ì ìš© (Gradient Boosting)

```python
from sklearn.ensemble import GradientBoostingClassifier

gbm = GradientBoostingClassifier(random_state=42)

score_gbm_cv = cross_val_score(gbm, X_train_cv, y_train, cv=5)
score_gbm_tv = cross_val_score(gbm, X_train_tv, y_train, cv=5)

print(score_gbm_cv.mean())
print(score_gbm_tv.mean())

```

ğŸ“Œ *Boosting ë°©ì‹ìœ¼ë¡œ í•˜ë‚˜ì˜ ëª¨ë¸ì´ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµ*

---

## âœ… í–¥í›„ í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤

- `XGBoost`, `LightGBM`ë„ ìœ ì‚¬í•˜ê²Œ ì ìš© ê°€ëŠ¥
- `GridSearchCV`ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- `confusion matrix`, `classification report`ë¡œ ëª¨ë¸ ì„±ëŠ¥ ì„¸ë¶€ í‰ê°€ ê°€ëŠ¥

---

- **í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸** ì „ë°˜ì„ ê²½í—˜
- ì‹¤ì œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ í”„ë¡œì íŠ¸ì˜ ì¢‹ì€ ê¸°ì´ˆ êµ¬ì¡°
- ì¶”ê°€ì ì¸ ì‹œê°í™”, ëª¨ë¸ ì•™ìƒë¸”, í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ ë“±ë„ ì´ì–´ì„œ ì ìš© ê°€ëŠ¥